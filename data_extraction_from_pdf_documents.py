# -*- coding: utf-8 -*-
"""Data Extraction from PDF Documents

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rCCf42wlxA-nMzbTFKqostSdk7yiyltm
"""

!pip install pdfplumber
!pip install pandas
!pip install matplotlib

import os
import re
import json
import logging
import pdfplumber
import pandas as pd
import matplotlib.pyplot as plt
from zipfile import ZipFile

# Suppress pdfminer/pdfplumber warnings
logging.getLogger("pdfminer").setLevel(logging.ERROR)

# Detect if in Google Colab
try:
    from google.colab import files
    IN_COLAB = True
except ImportError:
    IN_COLAB = False

# Output directories
OUTPUT_DIR = "output"
TABLE_DIR = os.path.join(OUTPUT_DIR, "tables")
KV_DIR = os.path.join(OUTPUT_DIR, "key_values")
os.makedirs(TABLE_DIR, exist_ok=True)
os.makedirs(KV_DIR, exist_ok=True)

def extract_key_value_pairs(text):
    patterns = [
        r"(Invoice\s*Number|Invoice\s*No|Bill\s*No)[:\-]?\s*(\S+)",
        r"(Date|Invoice\s*Date)[:\-]?\s*([\d/.-]+)",
        r"(Total\s*Amount|Amount\s*Due|Grand\s*Total|Total\s*Due)[:\-]?\s*\$?([\d,]+\.\d{2})"
    ]
    data = {}
    for pattern in patterns:
        matches = re.findall(pattern, text, flags=re.IGNORECASE)
        for match in matches:
            key = match[0].strip()
            value = match[1] if len(match) == 2 else match[2]
            data[key] = value
    return data

def clean_table(df):
    df = df.loc[:, ~df.columns.duplicated()]
    for col in df.columns:
        if df[col].dtype == object:
            df[col] = df[col].map(lambda x: x.replace('\n', ' ') if isinstance(x, str) else x)
            df[col] = df[col].map(lambda x: x.strip() if isinstance(x, str) else x)
    return df

def process_pdf(pdf_path):
    print(f"\nProcessing {pdf_path} ...")
    all_tables = []

    with pdfplumber.open(pdf_path) as pdf:
        full_text = ""
        for page_num, page in enumerate(pdf.pages, start=1):
            text = page.extract_text() or ""
            full_text += text + "\n"
            tables = page.extract_tables()
            for t_idx, table in enumerate(tables, start=1):
                df = pd.DataFrame(table[1:], columns=table[0])
                df = clean_table(df)

                base_name = os.path.splitext(os.path.basename(pdf_path))[0]
                csv_filename = f"{base_name}_page{page_num}_table{t_idx}.csv"
                csv_path = os.path.join(TABLE_DIR, csv_filename)
                df.to_csv(csv_path, index=False)
                print(f" Saved table {t_idx} from page {page_num} as {csv_filename}")
                all_tables.append(df)

    # Extract key-value pairs
    extracted_key_values = extract_key_value_pairs(full_text)
    base_name = os.path.splitext(os.path.basename(pdf_path))[0]
    kv_json_path = os.path.join(KV_DIR, f"{base_name}_key_values.json")
    with open(kv_json_path, "w") as f:
        json.dump(extracted_key_values, f, indent=4)
    print(f" Saved key-value pairs to {kv_json_path}")

    return {
        "pdf_file": pdf_path,
        "key_values": extracted_key_values,
        "tables_count": len(all_tables),
        "tables": [df.to_dict(orient='records') for df in all_tables]
    }

def zip_output_folder():
    zip_path = "output.zip"
    with ZipFile(zip_path, 'w') as zipf:
        for root, dirs, files_in_dir in os.walk(OUTPUT_DIR):
            for file in files_in_dir:
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, OUTPUT_DIR)
                zipf.write(file_path, arcname)
    print(f"\nZipped output folder as {zip_path}")
    return zip_path

def main():
    pdf_files = [
        "/content/Sample PDF.pdf",
        "/content/Reports.pdf",
        "/content/Invoice.pdf",
        "/content/Form.pdf"
    ]

    all_results = []
    for pdf_path in pdf_files:
        if os.path.isfile(pdf_path) and pdf_path.lower().endswith(".pdf"):
            result = process_pdf(pdf_path)
            all_results.append(result)
        else:
            print(f"⚠️ Skipped invalid or missing path: {pdf_path}")

    # Save combined JSON results
    with open(os.path.join(OUTPUT_DIR, "extracted_data.json"), "w") as f:
        json.dump(all_results, f, indent=4)

    # Combine all tables to one CSV
    all_tables_dfs = []
    for result in all_results:
        for table_dict in result["tables"]:
            df = pd.DataFrame(table_dict)
            all_tables_dfs.append(df)

    if all_tables_dfs:
        combined_df = pd.concat(all_tables_dfs, ignore_index=True)
        combined_csv_path = os.path.join(OUTPUT_DIR, "extracted_tables_combined.csv")
        combined_df.to_csv(combined_csv_path, index=False)
        print(f"\nCombined CSV of all tables saved to {combined_csv_path}")
    else:
        print("No tables found in PDFs.")

    print("\n✅ All PDFs processed. Check the 'output' folder.")

    # Optional: Plot numeric column from first table
    if all_tables_dfs:
        first_df = all_tables_dfs[0]
        numeric_cols = first_df.select_dtypes(include='number').columns
        if not numeric_cols.empty:
            col = numeric_cols[0]
            plt.bar(first_df.index, first_df[col])
            plt.xlabel("Row Index")
            plt.ylabel(col)
            plt.title(f"Bar plot of column '{col}' from first table")
            plt.show()
        else:
            print("No numeric columns found in the first table for plotting.")

    # Zip the folder
    zip_path = zip_output_folder()

    # Auto-download in Colab
    if IN_COLAB:
        files.download(zip_path)

if __name__ == "__main__":
    main()